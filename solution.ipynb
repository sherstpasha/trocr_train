{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0701878",
   "metadata": {},
   "source": [
    "# TroCR Small Обучение на Кириллице\n",
    "\n",
    "Этот ноутбук содержит полный код для обучения модели TroCR (Transformer-based OCR) small на датасете с кириллическим текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0ba07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pasha\\trocr_train\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel, \n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Пути к данным\n",
    "DATA_DIR = Path(\"orig_cyrillic\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "TRAIN_TSV = DATA_DIR / \"train.tsv\"\n",
    "TEST_TSV = DATA_DIR / \"test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7ad2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyrillicOCRDataset(Dataset):\n",
    "    def __init__(self, tsv_file, img_dir, processor, max_target_length=128):\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.img_dir = Path(img_dir)\n",
    "        \n",
    "        # Загрузка данных из TSV\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t', header=None, names=['image', 'text'])\n",
    "        print(f\"Loaded {len(self.df)} samples from {tsv_file}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Загрузка изображения\n",
    "        img_name = self.df.iloc[idx]['image']\n",
    "        img_path = self.img_dir / img_name\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Возвращаем пустое изображение в случае ошибки\n",
    "            image = Image.new('RGB', (384, 384), (255, 255, 255))\n",
    "        \n",
    "        # Текст\n",
    "        text = str(self.df.iloc[idx]['text'])\n",
    "        \n",
    "        # Обработка изображения\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Токенизация текста\n",
    "        labels = self.processor.tokenizer(\n",
    "            text, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        # Замена padding токенов на -100 для игнорирования при вычислении loss\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": labels.squeeze()\n",
    "        }\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e3745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: microsoft/trocr-small-handwritten\n",
      "Vocab size: 64044\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели TroCR Small\n",
    "model_name = \"microsoft/trocr-small-handwritten\"\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Настройка параметров генерации\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "# Параметры beam search для инференса\n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69250d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 72286 samples from orig_cyrillic\\train.tsv\n",
      "Loaded 1544 samples from orig_cyrillic\\test.tsv\n",
      "Train dataset size: 72286\n",
      "Eval dataset size: 1544\n"
     ]
    }
   ],
   "source": [
    "# Создание тренировочного и валидационного датасетов\n",
    "train_dataset = CyrillicOCRDataset(\n",
    "    tsv_file=TRAIN_TSV,\n",
    "    img_dir=TRAIN_DIR,\n",
    "    processor=processor,\n",
    "    max_target_length=128\n",
    ")\n",
    "\n",
    "eval_dataset = CyrillicOCRDataset(\n",
    "    tsv_file=TEST_TSV,\n",
    "    img_dir=TEST_DIR,\n",
    "    processor=processor,\n",
    "    max_target_length=128\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e38aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Загрузка метрик\n",
    "cer_metric = load(\"cer\")\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # Декодирование предсказаний и меток\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Вычисление метрик\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc51eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr-small-cyrillic\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    log_level=\"info\",\n",
    "\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    "    gradient_accumulation_steps=2,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37294825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pasha\\trocr_train\\env\\Lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:139: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pasha\\AppData\\Local\\Temp\\ipykernel_10384\\1982507529.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 72,286\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 45,180\n",
      "  Number of trainable parameters = 61,596,672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer created. Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2448' max='45180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2448/45180 23:44 < 6:54:39, 1.72 it/s, Epoch 0.54/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.587000</td>\n",
       "      <td>4.573287</td>\n",
       "      <td>0.889811</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.271400</td>\n",
       "      <td>4.317388</td>\n",
       "      <td>0.879892</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.157200</td>\n",
       "      <td>4.068131</td>\n",
       "      <td>0.865520</td>\n",
       "      <td>1.150209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.144000</td>\n",
       "      <td>3.921929</td>\n",
       "      <td>0.904251</td>\n",
       "      <td>1.159944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1544\n",
      "  Batch size = 8\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 128,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "c:\\Users\\pasha\\trocr_train\\env\\Lib\\site-packages\\transformers\\generation\\utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./trocr-small-cyrillic\\checkpoint-500\n",
      "c:\\Users\\pasha\\trocr_train\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-500\\config.json\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-500\\generation_config.json\n",
      "Model weights saved in ./trocr-small-cyrillic\\checkpoint-500\\model.safetensors\n",
      "Image processor saved in ./trocr-small-cyrillic\\checkpoint-500\\preprocessor_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1544\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trocr-small-cyrillic\\checkpoint-1000\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-1000\\config.json\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-1000\\generation_config.json\n",
      "Model weights saved in ./trocr-small-cyrillic\\checkpoint-1000\\model.safetensors\n",
      "Image processor saved in ./trocr-small-cyrillic\\checkpoint-1000\\preprocessor_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1544\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trocr-small-cyrillic\\checkpoint-1500\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-1500\\config.json\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-1500\\generation_config.json\n",
      "Model weights saved in ./trocr-small-cyrillic\\checkpoint-1500\\model.safetensors\n",
      "Image processor saved in ./trocr-small-cyrillic\\checkpoint-1500\\preprocessor_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1544\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trocr-small-cyrillic\\checkpoint-2000\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-2000\\config.json\n",
      "Configuration saved in ./trocr-small-cyrillic\\checkpoint-2000\\generation_config.json\n",
      "Model weights saved in ./trocr-small-cyrillic\\checkpoint-2000\\model.safetensors\n",
      "Image processor saved in ./trocr-small-cyrillic\\checkpoint-2000\\preprocessor_config.json\n",
      "Deleting older checkpoint [trocr-small-cyrillic\\checkpoint-500] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "# Создание Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"Trainer created. Starting training...\")\n",
    "\n",
    "# Запуск обучения\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовом датасете\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation results:\")\n",
    "print(f\"CER (Character Error Rate): {eval_results['eval_cer']:.4f}\")\n",
    "print(f\"WER (Word Error Rate): {eval_results['eval_wer']:.4f}\")\n",
    "print(f\"Eval loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37343fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение обученной модели\n",
    "output_dir = \"./trocr-small-cyrillic-final\"\n",
    "trainer.save_model(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\nModel saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def predict_image(image_path, model, processor):\n",
    "    \"\"\"Предсказание текста на изображении\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Генерация текста\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Тестирование на случайных примерах из тестового набора\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing on random examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Перемещение модели на устройство\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Выбор 5 случайных примеров\n",
    "test_df = pd.read_csv(TEST_TSV, sep='\\t', header=None, names=['image', 'text'])\n",
    "random_indices = random.sample(range(len(test_df)), min(5, len(test_df)))\n",
    "\n",
    "for idx in random_indices:\n",
    "    img_name = test_df.iloc[idx]['image']\n",
    "    true_text = test_df.iloc[idx]['text']\n",
    "    img_path = TEST_DIR / img_name\n",
    "    \n",
    "    if img_path.exists():\n",
    "        predicted_text = predict_image(img_path, model, processor)\n",
    "        \n",
    "        print(f\"\\nImage: {img_name}\")\n",
    "        print(f\"True text:      {true_text}\")\n",
    "        print(f\"Predicted text: {predicted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        print(f\"Image {img_path} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path):\n",
    "    \"\"\"Загрузка обученной модели для инференса\"\"\"\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "    processor = TrOCRProcessor.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "def recognize_text(image_path, model, processor):\n",
    "    \"\"\"\n",
    "    Распознавание текста на изображении\n",
    "    \n",
    "    Args:\n",
    "        image_path: путь к изображению\n",
    "        model: обученная модель\n",
    "        processor: процессор для обработки изображений\n",
    "        \n",
    "    Returns:\n",
    "        str: распознанный текст\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "    \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Пример использования:\n",
    "# model, processor = load_trained_model(\"./trocr-small-cyrillic-final\")\n",
    "# text = recognize_text(\"path/to/image.png\", model, processor)\n",
    "# print(text)\n",
    "\n",
    "print(\"\\nИнференс функции готовы к использованию!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
